%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\addcontentsline{toc}{chapter}{\abstractname}

\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}
In this composition, we propose a novel Reinforcement Learning (\acs{RL}) algorithm consisting in a stochastic variance-reduced version of policy gradient to solve Markov Decision Processes (\acs{MDP}s).\newline
\ac{SVRG} methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective function; II) approximations in the \ac{FG} computation; and III) a non-stationary sampling process. The result is \ac{SVRPG}, a stochastic variance reduced policy gradient algorithm that leverages on importance weights to preserve the unbiasedness of the gradient estimate. Under standard assumptions on the \acs{MDP}, we provide convergence guarantees for \acs{SVRPG} with a convergence rate that is linear under increasing batch sizes. Finally, we suggest practical variants of \acs{SVRPG}, and we empirically evaluate them on continuous \acs{MDP}s.

\vfill
\newpage
\pdfbookmark[1]{Sommario}{Sommario}
\chapter*{Sommario}
In questo elaborato proponiamo un nuovo algoritmo di \ac{RL}. Questo algoritmo, consiste  nell'utilizzo di una tecnica chiamata \ac{SVRG} volta alla riduzione della varianza dello stimatore  utilizzato dal policy gradient nella risoluzione di processi decisionali di Markov (\acs{MDP}s).\newline
Algoritmi quali \acs{SVRG} hanno avuto molto successo nell'ambito dell' apprendimento supervisionato. Il suo utilizzo in contesti policy gradient non \`e banale e necessita di tenere in considerazione alcune cose: I) la funzione obiettivo non Ã¨ concava; II) il \ac{FG} deve essere necessariamente approssimato;  III) il processo di campionamento non \`e stazionario. Il risultato \`e \ac{SVRPG}, un algoritmo di riduzione della varianza del gradiente della politica che sfrutta gli importance weights per preservare la correttezza dello stimatore del gradiente stesso. Date le classiche assunzioni del \acs{MDP}, abbiamo fornito garanzie di convergenza per \acs{SVRPG} con un tasso di convergenza che \`e lineare al crescere della dimensione del batch. Infine abbiamo implementato una variante pratica di \acs{SVRPG}. Abbiamo, inoltre, valutato questa variante empiricamente  su alcuni \acs{MDP} continui.

\endgroup