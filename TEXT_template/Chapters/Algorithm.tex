\chapter{Stochastic Variance Reduced Policy Gradient} \label{chap:algorithm}
\vspace{-0.05in}
In online \acs{RL} problems, the usual approach is to manually tune the batch size of \acs{SG} to find the optimal trade-off between variance and speed.
Recall that, compared to \acs{SL}, the samples are not fixed in advance but we need to collect them at each policy update.
Since this operation may be costly, we would like to minimize the number of interactions with the environment.
For these reasons, we would like to apply \acs{SVRG} to \acs{RL} problems in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence.
However, as previously introduced, a direct application of \acs{SVRG} to \acs{RL} is not possible due to the following issues:
% In on-line policy gradient problems, SG is not really a choice, but is dictated by the necessity of interacting with an unknown environment.\todopir{I do not understand the meaning of this sentence. I would say that SG is not a choice by referring to papers that use batches}
% We would like to apply SVRG to the policy gradient framework in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence. In this scenario, the function $f(\vtheta)$ to optimize is $J(\vtheta)$ and the (implicit) dataset $\mathcal{D}$ is the set of all possible trajectories $\mathcal{T}$. Compared to the typical finite sum optimization scenario, we have the following additional challenges:
\begin{description}
	\item[\textbf{\MakeUppercase{N}on-concavity}:] the objective function $J(\vtheta)$ is typically non-concave.
	\item[\textbf{\MakeUppercase{I}nfinite dataset}:] in continuous \acs{RL} tasks the \acs{RL} optimization cannot be expressed as a finite-sum problem. The objective function is an expected value over the trajectory density $p_{\vtheta}(\tau)$ of the total discounted reward, for which we would need an infinite dataset. Whereas in discrete \acs{RL} tasks, even though we can express the optimization problem as a finite-sum we can't handle it anyway because, due to combinatorial reasons, the number of addends appearing in the sum itself becomes easily intractable.
	%    \item[Approximation:] the dataset $\mathcal{T}$ can be infinite, but we can only sample a finite number of trajectories;
	\item[\textbf{\MakeUppercase{N}on-stationarity}:] the distribution of the samples changes over time. In particular, the value of the policy parameter $\vtheta$ influences the sampling process. More precisely, we need to resample after any update of the policy itself, which implies using different distributions to colect samples between different updates of the policy.
\end{description}
To deal with non-concavity, we require $J(\vtheta)$ to be $L$-smooth, which is a reasonable assumption for common policy classes such as Gaussian\footnote{See Section \ref{sec:gaussianassumption} in \hyperref[chap:convergence]{Chapter 4} for more details on the Gaussian policy case.} and softmax~\citep[\eg][]{Furmston2012unifying,pirotta2015lipschitz}.
Because of the infinite dataset, we can only rely on an estimate of the full gradient.
\citet{harikandeh2015stopwasting} analysed this scenario under the assumptions of $h$ being concave, showing that \acs{SVRG} is robust to an inexact computation of the full gradient. In particular, it is still possible to recover the original convergence rate if the error decreases at an appropriate speed. In \hyperref[chap:convergence]{Chapter 4} we will show how the estimation accuracy impacts on the convergence results with a non-concave objective.
Finally, the non-stationarity of the optimization problem introduces a bias into the \acs{SVRG} estimator in Eq.~\eqref{E:svrg.gradient.correction}.
To overcome the latter issue we employ importance weighting~\citep[\eg][]{rubinstein1981simulation,precup2000eligibility} to correct the distribution shift.
%Finally, we employ importance weighting~\citep[\eg]{rubinstein1981simulation,precup2000eligibility} to guarantee proper sampling of trajectories.


\begin{algorithm}[h]
	\caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, batch size $N$, mini-batch size $B$, gradient estimator $g$, initial parameter $\vtheta_{m}^0 := \wt{\vtheta}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE $\vtheta_0^{s+1} := \wt{\vtheta}^{s} = \vtheta_{m}^s$
		\STATE Sample $N$ trajectories $\{\tau_j\}$ from $p(\cdot\vert\wt{\vtheta}^{s})$
		\STATE $ \wt{\mu} = \gradApp{\wt{\vtheta}^{s}}{N}$ (see Eq.~\eqref{E:policygradient.estimate})% = \frac{1}{N}\sum_{j=0}^{N-1}g(\tau_j | \wt{\vtheta}^s)$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Sample $B$ trajectories $\{\tau_i\}$ from $p(\cdot\vert\vtheta_t^{s+1})$
		%        \STATE $c^{s+1}_t = \frac{1}{B} \sum\limits_{i=0}^{B-1} \left( g(\tau_i|\vtheta_t^{s+1}) - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \right)$
		\STATE $c^{s+1}_t = \frac{1}{B} \sum\limits_{i=0}^{B-1}
		\begin{aligned}[t]
		\Big( & g(\tau_i|\vtheta_t^{s+1})\\ 
		& \quad{} - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \Big)
		\end{aligned}$
		%        \STATE $\begin{aligned}[t]
		%                &v^{s+1}_t = \wt{\mu}\\ 
		%                & +\frac{1}{B} \sum\limits_{i=0}^{B-1} \left( g(\tau_i|\vtheta_t^{s+1}) - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \right)
		%                \end{aligned}$
		\STATE $v^{s+1}_t = \wt{\mu} + c^{s+1}_t$ % \gradApp{\wt{\vtheta}^{s}}{N} + c^{s}_t$
		%%		\begin{align*}
		%%		\blacktriangledown J(\vtheta_t^{s+1}) = 
		%%		&\gradApp{\wt{\vtheta}^s}{N} \\
		%%		&+\frac{1}{B}\sum_{i=0}^{B-1}\left[ 
		%%		\score{}{\tau_i\vert\vtheta_t^{s+1}}\Reward(\tau_i)\right. \\
		%%		&\left. - \omega(\tau_i)\score{}{\tau_i \vert \wt{\vtheta}^{s}}\Reward(\tau_i)\right]
		%%		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha v^{s+1}_t$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries return} $\vtheta_A\coloneqq\vtheta_t^{s+1}$ with $(s,t)$ picked uniformly at random from $\{[0,S-1]\times[0,m-1]\}$
	\end{algorithmic}
\end{algorithm} 

We can now introduce \acs{SVRPG} for a generic policy gradient estimator $g$ (\eg REINFORCE or G(PO)MDP). Pseudo-code is provided in Algorithm \ref{alg:svrpg}.
The overall structure is the same as Algorithm \ref{alg:svrg}, 
but the snapshot gradient is not exact and the gradient estimate used between snapshots is corrected using importance weighting:\footnote{Note that $g$ can be any unbiased estimator, with or without baseline. The unbiasedness is required for theoretical results.}
\begin{align*}
\blacktriangledown J(\vtheta_{t}) &= \wh{\nabla}_N J(\wt{\vtheta}) + g(\tau|\vtheta_t) - \omega(\tau|\vtheta_t, \wt{\vtheta}) g(\tau|\wt{\vtheta})
\end{align*}
for any $t \in \{0,\ldots,m-1\}$,
where $\wh{\nabla}_N J(\wt{\vtheta})$ is as in Eq.~\eqref{E:policygradient.estimate} where $\mathcal{D}_N$ is sampled using the snapshot policy $\pi_{\wt{\vtheta}}$, $\tau$ is sampled from the current policy $\pi_{\vtheta_t}$, and $\omega(\tau|\vtheta_t, \wt{\vtheta}) = \frac{p(\tau|\wt{\vtheta})}{p(\tau|\vtheta_t)}$ is an importance weight from $\pi_{\vtheta_t}$ to the snapshot policy $\pi_{\wt{\vtheta}}$. 
Similarly to \acs{SVRG}, we have that $\vtheta_0 := \wt{\vtheta}$, and the update is a \acs{FG} step.
Our update is still fundamentally on-policy since the weighting concerns only the correction term. However, this partial ``off-policyness'' represents an additional source of variance. This is a well-known issue of importance sampling~\citep[\eg][]{thomas2015high}. To mitigate it, we use mini-batches of trajectories of size $B \ll N$ to average the correction, \ie
\begin{align}\label{E:svrpg.estimate.batch}
\blacktriangledown J(\vtheta_{t}) &:= v_t= \wh{\nabla}_N J(\wt{\vtheta})\\ \notag
& \quad{} + 
\underbracket{
	\frac{1}{B} \sum_{i=0}^{B-1} \left[
	g(\tau_i|\vtheta_t) - \omega(\tau_i|\vtheta_t, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})
	\right]}_{c_t}.
\end{align}
This gradient estimator is computed in Algorithm \ref{alg:svrpg} at lines 5, 7, 8 and 9.\newline
It is worth noting that the full gradient and the correction term have the same expected value:
\[
\ \EVV[\tau_i \sim p(\cdot|\vtheta_t)]{\frac{1}{B} \sum_{i=0}^{B-1} \omega(\tau_i|\vtheta_t, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})} = \nabla J(\wt{\vtheta}).
\]
This property will be used to prove Lemma~\ref{L:svrpg.properties}. The reader can refer to \hyperref[chap:art]{Chapter 2} for off-policy gradients and variants of REINFORCE and G(PO)MDP.
%\begin{align*}
%&\blacktriangledown J(\vtheta) = \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \vert \wt{\vtheta})\Reward(\tau_j) +\\
%&\quad\frac{1}{B}\left[\sum_{i=0}^{B-1}
%\nabla\log\pi(\tau_i \vert \vtheta)\Reward(\tau_i) 
%- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \wt{\vtheta})\Reward(\tau_i)\right].
%\end{align*}
The use of mini-batches is also common practice in \acs{SVRG} since it can yield a performance improvement even in the supervised case~\citep{harikandeh2015stopwasting,konevcny2016mini}. It is easy to show that the \acs{SVRPG} estimator has the following, desirable properties:
\begin{lemma}\label{L:svrpg.properties}
	Let $\wh{\nabla}_N J(\vtheta)$ be an unbiased estimator of~\eqref{E:policygradient}
	%Denote by $\wh{\nabla}_N^{\textsc{RF}} J(\vtheta)$ the REINFORCE estimator in~\eqref{E:policygradient.estimate} 
	and let $\vtheta^* \in \argmin_{\vtheta} \{J(\vtheta)\}$. Then, the \acs{SVRG} estimate in~\eqref{E:svrpg.estimate.batch} is \emph{unbiased}
	\begin{equation}\label{eq:unbiased}
	\mathop{\mathbb{E}}
	\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
	\end{equation}
	and regardless of the mini-batch size $B$:
	\begin{equation}\label{eq:zerovar}
	\Var\left[\gradBlack{\vtheta^*}\right] = 
	\Var\left[\wh{\nabla}_N J(\vtheta^*)\right].
	\end{equation}
	For any vector $\mathbf{x}$, we use $\Var[\mathbf{x}]$ to denote the trace of the covariance matrix, \ie $\Tr\EVV[]{(\mathbf{x}-\EVV[]{\mathbf{x}})(\mathbf{x}-\EVV[]{\mathbf{x}})^T}$.
\end{lemma}
%\begin{proof}
%Claim (\ref{eq:unbiased}) is from the unbiasedness of the estimators:
%\begin{align*}
%\EVV[]{\gradBlack{\vtheta}} &= \EVV[]{\gradApp{\wt{\vtheta}}{N}}  + \EVV[]{\gradApp{\vtheta}{B}} \\
%&\qquad- \EVV[]{\frac{1}{B}\sum_{i=0}^{B-1}\omega(\tau_i|\vtheta, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})} \\
%&= \gradJ{\wt{\vtheta}} + \gradJ{\vtheta} - \gradJ{\wt{\vtheta}} = \gradJ{\vtheta}.
%\end{align*}
%As for claim (\ref{eq:zerovar}), note that as $\vtheta\to\vtheta^*$, also $\wt{\vtheta}\to\vtheta^*$. Hence, by continuity of $J(\vtheta)$:
%\begin{align*}
%\Var\left[\gradBlack{\vtheta}\right] &\to \Var\left[\gradApp{\vtheta^*}{N}\right] + \frac{1}{B}\Var\left[g(\tau|\vtheta^*)\right. \\
%&\left.- \cancel{\omega(\tau|\vtheta^*,\vtheta^*)}g(\tau|\vtheta^*)\right]
%= \Var\left[\gradApp{\vtheta^*}{N}\right].
%\end{align*}
%Note that it is essential that the trajectories used in the second and the third term are the same for the variance to vanish.
%\end{proof}
Previous results hold for both REINFORCE and G(PO)MDP.
In particular, the latter result suggests that an \acs{SVRG}-like algorithm using $\gradBlack{\vtheta}$ can achieve faster convergence, by performing much more parameter updates with the same data without introducing additional variance (at least asymptotically).
Note that the randomized return value of Algorithm \ref{alg:svrpg} does not affect online learning at all, but will be used as a theoretical tool in the next chapter.

\vspace{-0.05in}
