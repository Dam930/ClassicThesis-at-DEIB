\chapter{Conclusions} \label{chap:conclusions}
\section{Related Work}
\vspace{-0.05in}
Despite the considerable interest received in SL, variance-reduced gradient approaches have not attracted the RL community.
As far as we know, there are just two applications of SVRG in RL.
The first approach~\citep{du2017svrgpe} aims to exploit SVRG for policy evaluation.
The policy evaluation problem is more straightforward than the one faced in this paper (control problem).
In particular, since the goal is to evaluate just the performance of a predefined policy, the optimization problem is stationary.
The setting considered in the paper is the one of policy evaluation by minimizing the empirical mean squared projected Bellman error (MSPBE) with a linear approximation of the value function. \citet{du2017svrgpe} shown that this problem can be equivalently reformulated as a convex-concave saddle-point problem that is characterized by a finite-sum structure.
This problem can be solved using a variant of SVRG~\citep{Palaniappan2016svrgsaddle} for which convergence guarantees have been provided.
The second approach~\citep{xu2017svrgtrpo} uses SVRG as a practical method to solve the optimization problem faced by Trust Region Policy Optimization (TRPO) at each iteration. This is just a direct application of SVRG to a problem having finite-sum structure since no specific structure of the RL problem is exploited.
It is worth to mention that, for practical reasons, the authors proposed to use a Newton conjugate gradient method with SVRG.

In the recent past, there has been a surge of studies investigating variance reduction techniques for policy gradient methods.
% The most widespread technique of reducing the variance preserving the unbiasedness of the gradient is through baselines~\citep[\eg][]{williams1992simple,weaver2001optimal,peters2008reinforcement,Thomas2017actionbaseline,wu2018variance}.
The specific structure of the policy gradient allows incorporating a baseline (\ie a function $b :\mathcal{S}\times\mathcal{A} \to \realspace$) without affecting the unbiasedness of the gradient~\citep[\eg][]{williams1992simple,weaver2001optimal,peters2008reinforcement,Thomas2017actionbaseline,wu2018variance}.
Although the baseline can be arbitrarily selected, literature often refers to the optimal baseline as the one minimizing the variance of the estimate.
Nevertheless, even the baseline needs to be estimated from data. This fact may partially reduce its effectiveness by introducing variance.
Even if these approaches share the same goal as SVRG, they are substantially different.
In particular, the proposed SVRPG does not make explicit use of the structure of the policy gradient framework, and it is independent of the underlying gradient estimate (\ie with or without baseline).
This suggests that would be possible to integrate an ad-hoc SVRPG baseline to further reduce the variance of the estimate.
Since this paper is about the applicability of SVRG technique to RL, we consider this topic as future work.
Additionally, the experiments show that SVRPG has an advantage over G(PO)MPD even when the baseline is used (see the half-cheetah domain in Section~\ref{sec:exp}).

Concerning importance weighting techniques, RL has made extensive use of them for off-policy problems~\citep[\eg][]{precup2000eligibility,thomas2015high}. However, as mentioned before, SVRPG cannot be compared to such methods since it is in all respects an on-policy algorithm. Here, importance weighting is just a statistical tool used to preserve the unbiasedness of the corrected gradient.

\vspace{-0.05in}

\section{discussion}
\vspace{-0.05in}
In this paper, we introduced SVRPG, a variant of SVRG designed explicitly for RL problems.
The control problem considered in the paper has a series of difficulties that are not common in SL.
Among them, non-concavity and approximate estimates of the FG have been analysed independently in SL~\citep[\eg][]{allen2016variance,reddi2016stochastic,harikandeh2015stopwasting} but never combined.
Nevertheless, the main issue in RL is the non-stationarity of the sampling process since the distribution underlying the objective function is policy-dependent.
We have shown that by exploiting importance weighting techniques, it is possible to overcome this issue and preserve the unbiasedness of the corrected gradient.
We have additionally shown that, under mild assumptions that are often verified in RL applications, it is possible to derive convergence guarantees for SVRPG.
%{\color{red}SVRG has been previously used in RL for policy evaluation~\citep{du2017svrgpe} and as alternative optimization method for TRPO~\citep{xu2017svrgtrpo}.
%The first case is much more similar to the SL case since the problem is stationary and may be made concave, removing several issues we have faced here.
%In the second case, it was used as a better optimization technique to solve the per-iteration problem imposed by TRPO.
%}
%\todomatout{Repetition of Section 6?}
Finally, we have empirically shown that practical variants of the theoretical SVRPG version can outperform classical actor-only approaches on benchmark tasks.
Preliminary results support the effectiveness of SVRPG also with a commonly used baseline for the policy gradient.
Despite that, we believe that it will be possible to derive a baseline designed explicitly for SVRPG to exploit the RL structure and the SVRG idea jointly.
Another possible improvement would be to employ the natural gradient~\cite{kakade2002natural} to better control the effects of parameter updates on the variance of importance weights. Future work should also focus on making batch sizes $N$ and $B$ adaptive, as suggested in~\cite{papini2017adaptive}.
\vspace{-0.05in}