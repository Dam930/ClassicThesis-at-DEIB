% !TEX root = ../ClassicThesis_DEIB.tex

\chapter{INTRODUCTION} \label{chap:aChapter}

\acf{RL} is a field of machine learning that aims at building intelligent machines, called agents, capable of learning complex tasks from experience.
The tasks in \acs{RL} are represented as a \acf{MDP}s \citep{puterman1990markov}. 
The goal of \acs{RL} \citep{sutton1998reinforcement} algorithms is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal.
This makes \acs{RL} fundamentally different from \acf{SL}, where correct actions are explicitly prescribed by a human teacher (e.g., for classification, in the form of class labels).
However, the two approaches share many challenges and tools: the problem of estimating a model from samples, which is at the core of \acs{SL}, is equally fundamental in \acs{RL}, whether we choose to model the environment, a value function, or directly a policy defining the agent\textquotesingle s behaviour.
A subset of \acs{RL} problems is composed of continous domain tasks: the states that the agent can observe and the possible actions it can perform are infinitely many.
\acs{RL} continuous domains are common in automatic control and robotic applications (\eg the state is the joint angle and the action is the torque on the joint). Traditional \acs{RL} algorithms, based on value functions, suffer from many problems when applied to continuous tasks, the most serious being the lack of strong theoretical guarantees. Policy gradient methods, besides guaranteeing convergence to locally optimal policies \citep{sutton2000policy}, are able to address many of the difficulties that characterize complex control problems, such as high dimensional state and action spaces. 
Many different policy gradient algorithms have been proposed (\eg REINFORCE \citep{williams1992simple} GPOMDP \citep{baxter2001infinite} etc.), all sharing the same basic principle of computing the gradient of a performance measure to update the agent\textquotesingle s policy in a direction of improvement. These methods have been successfully applied to complex control tasks \citep{deisenroth2013survey}. In most practical cases policy gradient is used to improve an existing policy, which may be designed by a human expert or learned by simulation.\newline
Computing a full estimate of the policy gradient is too expensive, hence it is usually employed \acf{SG} ascent \citep{nesterov2013introductory} whose idea in a generic Machine Learning task (\acs{SL},\acs{RL}...) is to iteratively focus on a random subset of the available data (batch) to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data.
However, the sub-sampling of data is a source of variance that can potentially compromise convergence, so that variance-reduced gradient algorithms such as \acf{SAG} \citep{roux2012stochastic}, \acf{SVRG} \citep{allen2016variance} and \acf{SAGA} \citep{defazio2014saga} have been introduced in the \acs{SL} framework. These new methods have already proven to be very successful in \acs{SL}. Therefore, moving them in the \acs{RL} framework would create an orthogonal way to reduce the variance of the policy gardient estimators \wrt traditional variance reduction \acs{RL} techniques.
\section{Goals, Motivations and Issues}
In \acs{RL} continous tasks it is difficult to estimate the policy gradient from the few samples we can obtain in a reasonable time: the set of possible data may even be infinite and the cost of sampling data is high because we need to interact with the environment which can be a physical simulator or even directly a robot.
Until now, \acs{SG} is the most used way for policy gradient estimation. Unfortunately, in the \acs{RL} framework, \acs{SG} suffer a huge variance because it uses Monte-Carlo estimates of the gradient when, theoretically, we should compute the gradient over an infinite dataset which, of course, is not feasible. Increasing the batch size \ie using more Monte-Carlo trajectories, is usually not an option in \acs{RL} due to the high cost of collecting samples, i.e., by interacting with the environment. For this reason, literature has focused on the introduction of baselines aiming to reduce the variance of the policy gradient estimator exploiting information of other known quantities (\cite{williams1992simple};\cite{peters2008reinforcement};\cite{thomas2017policy}).\newline 
As mentioned before, compared to other applications of \acs{SG}, the cost of collecting samples in \acs{RL} can be very high since it requires to interact with the environment.
This makes stochastic variance-reduced gradient methods potentially much more efficient than, e.g., batch learning. 
Unfortunately, \acs{RL} has a series of difficulties that are not present in \acs{SL}. First, in \acs{SL} the objective can often be designed to be strongly concave (we aim to maximize). This is not the case for \acs{RL}, so we have to deal with non-concave objective functions. Then, the dataset is not initially available and may even be infinite, which makes approximations unavoidable. This rules out \acs{SAG} and \acs{SAGA} because of their storage requirements, which leaves \acs{SVRG} as the most promising choice in order to reduce the variance of policy gradient estimators.
Finally, the distribution used to sample data is not under direct control of the algorithm designer, but it is a function of policy parameters that changes over time as the policy is optimized, which is a form of non-stationarity.
\section{Original Contribution}
This composition develops a novel approach to reduce the estimation variance of the policy gradient adapting \acs{SVRG} to the \acs{RL} framework.\newline 
\acs{SVRG} has been used in \acs{RL} as an efficient technique to optimize the per-iteration problem in \ac{TRPO} \citep{xu2017stochastic} or for policy evaluation \citep{du2017stochastic}. In both cases, the optimization problems dealt with resemble the \acs{SL} scenario and are not affected by all the previously mentioned issues.
We will give some theoretical fundamentals for our \acs{SVRG} version, called \acs{SVRPG}, in particular, we will provide convergence guarantees for the proposed algorithm under proper assumptions. Non-concavity and approximate estimates of the \acf{FG} have been analysed independently in \acs{SL} (\cite{allen2016variance},\cite{reddi2016stochastic},\cite{harikandeh2015stopwasting}) but never combined.\newline \acf{SVRPG} has a convergence rate of $O(\nicefrac{1}{T})$ where $T$ is the number of iterations. Up to now (e.g. in \acs{SG}) the convergence rate was  $O(\nicefrac{1}{\sqrt{T}})$ so this result is of great impact in theoretical \acs{RL}.\newline
 Furthermore, we will propose a practical variant of \acs{SVRPG} and we will show the empirical results of our work in widely known tasks from the contionuous \acs{RL} literature. We will compare our algorithm with traditional approaches which simply use \acs{SG}, showing that, in practice, our technique achieves better performace and the learning process is more stable.\newline
Our main contribution is a totally new technique for variance reduction in policy gradient algorithms which, as previously said, is orthogonal to the already existing ones (e.g. baselines).
Preliminary results support the effectiveness of \acs{SVRPG} also with a commonly used baseline for the policy gradient. Despite that, we believe that in future work it will be possible to derive a baseline designed explicitly for \acs{SVRPG} to jointly exploit the \acs{RL} structure and the \acs{SVRG} idea.\newline
This work also suggests new directions towards which we can push existing policy gradient optimization techniques stemming different new algorithms beyond those ones proposed in this composition: applying \acs{SVRPG} over natural gradient techniques, finding a proper way to build a critic suited for \acs{SVRPG} techniques. We expect new efforts coming in the future exploiting our theoretical foundations in order to extend our work along the prevously mentioned directions .\newline
\section{Document Structure}
The remaining part of this document is organized as follows:\newline
\newline
\hyperref[chap:art]{Chapter 2} describes all the state of the art theory and techniques needed to introduce our contribution. We will mainly talk about policy gradient, natural policy gradient and \acs{SVRG} in \acs{SL}.\newline
\hyperref[chap:algorithm]{Chapter 3} describes \acs{SVRPG} in detail, addressing all the difficulties mentioned above.\newline
\hyperref[chap:convergence]{Chapter 4} provides convergence guarantees under proper assumptions.\newline
\hyperref[chap:experiments]{Chapter 5} shows an empirical analysis of the proposed technique in continuous control benchmarks comparing the results with plain \acs{SG}.\newline
\hyperref[chap:conclusions]{Chapter 6} gives a discussion about the whole work providing also some hints about future development.
