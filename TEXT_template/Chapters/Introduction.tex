% !TEX root = ../ClassicThesis_DEIB.tex

\chapter{INTRODUCTION} \label{chap:aChapter}

\ac{RL} is a field of machine learning that aims at building intelligent machines, called agent, capable of learning complex tasks from experience.
The tasks in \acs{RL} are represented as a \acs{MDP}s. 
The goal of \acs{RL} \citep{sutton1998reinforcement} algorithm is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal.
This makes \acs{RL} fundamentally different from \ac{SL}, where correct actions are explicitly prescribed by a human teacher (e.g., for classification, in the form of class labels).
However, the two approaches share many challenges and tools: the problem of estimating a model from samples, which is at the core of \acs{SL}, is equally fundamental in \acs{RL}, whether we choose to model the environment, a value function, or directly a policy defining the agent’s behaviour.
A subset of \acs{RL} tasks is composed by continous domains tasks: the states in wich the agent can be and the possible actions it can perform are infinite.
\acs{RL} continuous domains are common in automatic control and robotic applications. Traditional \acs{RL} algorithms, based on value functions, suffer from many problems when applied to continuous tasks, the most serious being the lack of strong theoretical guarantees. Policy gradient methods, besides guaranteeing convergence to locally optimal policies \citep{sutton2000policy}, are able to address many of the difficulties that characterize complex control problems, such as high dimensional state that leads to an infinite space of the cases of interest. 
Many different policy gradient algorithms have been proposed, all sharing the same basic principle of computing the gradient of a performance measure to update the agent’s policy in a direction of improvement. These methods have been successfully applied to complex control tasks \citep{deisenroth2013survey}. Policy gradient is used to improve an existing policy, which may be designed by a human expert or learned by simulation.\newline
In a generic Machine Learning task (\acs{SL},\acs{RL}...), the idea of stochastic gradient (\acs{SG}) ascent \citep{nesterov2013introductory} is to iteratively focus on a random subset of the available data (batch) to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data.
However, the sub-sampling of data is a source of variance that can potentially compromise convergence, so that per-iteration efficiency and convergence rate must be traded off with proper handling of meta-parameters: in \acs{RL} continous task, for example, is difficult to estimate the policy gradient from the few sample we can obtain:  the set of data may even be infinite and the cost for sampling data is high.
Among RL approaches, policy gradient is the one that bears the closest similarity to \acs{SL} solutions. The fundamental principle of these methods is to optimize a parametric policy through \acs{SG}.
Until now, (\acs{SG}) is the unique way used for estimating policy gradient. The approach of increasing the batch size is not an option in \acs{RL} due to the high cost of collecting samples, i.e., by interacting with the environment. In \acs{SL} tasks there are variance-reduced gradient algorithms already used as \acs{SAG} \citep{roux2012stochastic}, \acs{SVRG} \citep{allen2016variance} and \acs{SAGA} \citep{defazio2014saga}. This technique that has proven to be very successful in \acs{SL}.\newline
The aim of this thesis is to develop a novel approach for reducing the variance of policy gradient's estimate using the \acs{SVRG} adapting it for \acs{RL} tasks.\newline
As mentioned before, compared to other applications of \acs{SG}, the cost of collecting samples can be very high since it requires to interact with the environment.
This makes SVRG-like methods potentially much more efficient than, e.g., batch learning. 
Unfortunately, RL has a series of difficulties that are not present in \acs{SL}. First, in \acs{SL} the objective can often be designed to be strongly concave (we aim to maximize). This is not the case for \acs{RL}, so we have to deal with non-concave objective functions. Then, as mentioned before, the dataset is not initially available and may even be infinite, which makes approximations unavoidable. This rules out \acs{SAG} and \acs{SAGA} because of their storage requirements, which leaves \acs{SVRG} as the most promising choice.
Finally, the distribution used to sample data is not under direct control of the algorithm designer, but it is a function of policy parameters that changes over time as the policy is optimized, which is a form of non-stationarity. 
SVRG has been used in \acs{RL} as an efficient technique for optimizing the per-iteration problem in Trust-Region Policy Optimization \citep{xu2017stochastic} or for policy evaluation \citep{du2017stochastic}. In both the cases, the optimization problems faced resemble the \acs{SL} scenario and are not affected by all the previously mentioned issues.
Our thesis makes a theoretical fundamentals of our \acs{SVRG} version, called \acs{SVRPG}, in particular we will provide a convergence guarantees of our proposed algorithm under the assumption given before. Furthermore, we propose a practical variants of \acs{SVRPG} and we show the empirical results of our work in task known in literature. We will compare our algorithm with traditional approaches with \acs{SG}. We propose some empirical variants that make some improvement.
Our contribution bring about a further technique for variance reduction in policy gradiend in addition to those existing, e.g. baseline.  Furthermore, we will show that the proposed \acs{SVRPG} algorithm can be plugged into a policy gradient estimate using baselines for exploit both thecniques at the same time.\newline
The structure of the remaining part of this document is as follows:
\hyperref[chap:art]{Chapter 2} describes the state of art of Policy gradient algorithms, of natural gradient and of the tecniques of stochastic variance-reduced gradient in \acs{SL}.\newline
\hyperref[chap:art]{Chapter 3} describes SVRPG, our proposed algorithm in detail addressing all the difficulties mentioned above. chapter also describes all SVRPG's variants.\newline
\hyperref[chap:art]{Chapter 4} provides the studied convergence guarantees under the presented assumption.\newline
\hyperref[chap:art]{Chapter 5} shows the empirical results in control tasks and comparison with \acs{SG}.\newline
\hyperref[chap:art]{Chapter 6} provides the comment of our work, the future work and the releted work.\newline
\hyperref[chap:art]{Appendix} provides all the dimostrations and the experimental details.
