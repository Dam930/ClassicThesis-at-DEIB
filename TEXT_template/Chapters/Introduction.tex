% !TEX root = ../ClassicThesis_DEIB.tex

\chapter{INTRODUCTION} \label{chap:aChapter}

\ac{RL} is a field of machine learning that aims at building intelligent machines, called agents, capable of learning complex tasks from experience.
The tasks in \acs{RL} are represented as a \acs{MDP}s. 
The goal of \acs{RL} \citep{sutton1998reinforcement} algorithms is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal.
This makes \acs{RL} fundamentally different from \ac{SL}, where correct actions are explicitly prescribed by a human teacher (e.g., for classification, in the form of class labels).
However, the two approaches share many challenges and tools: the problem of estimating a model from samples, which is at the core of \acs{SL}, is equally fundamental in \acs{RL}, whether we choose to model the environment, a value function, or directly a policy defining the agent’s behaviour.
A subset of \acs{RL} tasks is composed of continous domain tasks: the states in wich the agent can be and the possible actions it can perform are infinite.
\acs{RL} continuous domains are common in automatic control and robotic applications. Traditional \acs{RL} algorithms, based on value functions, suffer from many problems when applied to continuous tasks, the most serious being the lack of strong theoretical guarantees. Policy gradient methods, besides guaranteeing convergence to locally optimal policies \citep{sutton2000policy}, are able to address many of the difficulties that characterize complex control problems, such as high dimensional state that leads to an infinite space of the cases of interest. 
Many different policy gradient algorithms have been proposed, all sharing the same basic principle of computing the gradient of a performance measure to update the agent’s policy in a direction of improvement. These methods have been successfully applied to complex control tasks \citep{deisenroth2013survey}. Policy gradient is used to improve an existing policy, which may be designed by a human expert or learned by simulation.\newline
In a generic Machine Learning task (\acs{SL},\acs{RL}...), the idea of stochastic gradient (\acs{SG}) ascent \citep{nesterov2013introductory} is to iteratively focus on a random subset of the available data (batch) to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data.
However, the sub-sampling of data is a source of variance that can potentially compromise convergence, so that per-iteration efficiency and convergence rate must be traded off with proper handling of meta-parameters. In \acs{RL} continous tasks is difficult to estimate the policy gradient from the few sample we can obtain: the set of data may even be infinite and the cost to sample data is high.
Among RL approaches, policy gradient is the one that bears the closest similarity to \acs{SL} solutions. The fundamental principle of these methods is to optimize a parametric policy through \acs{SG}.
Until now, (\acs{SG}) is the unique way used for policy gradient estimation. In the \acs{RL} framework \acs{SG} suffer a huge variance because it uses Monte-Carlo estimates of the gradient when, theoretically, we should compute the gradient over an infinite dataset which, of course, is not feasible. Increasing the batch size (using more Monte-Carlo samples) is not an option in \acs{RL} due to the high cost of collecting samples, i.e., by interacting with the environment. For this reason, literature has focused on the introduction of baselines  (\ie functions $b : \mathcal{S} \times \mathcal{A} \to \realspace$) aiming to reduce the variance (\cite{williams1992simple};\cite{peters2008reinforcement};\cite{thomas2017policy}).\newline In \acs{SL} tasks there are variance-reduced gradient algorithms already used as \acs{SAG} \citep{roux2012stochastic}, \acs{SVRG} \citep{allen2016variance} and \acs{SAGA} \citep{defazio2014saga}. This techniques have proven to be very successful in \acs{SL}.\newline
This composition develops a novel approach to reduce the estimation variance of the policy gradient adapting \acs{SVRG} to \acs{RL}.\newline
As mentioned before, compared to other applications of \acs{SG}, the cost of collecting samples can be very high since it requires to interact with the environment.
This makes \ac{SVRG}-like methods potentially much more efficient than, e.g., batch learning. 
Unfortunately, RL has a series of difficulties that are not present in \acs{SL}. First, in \acs{SL} the objective can often be designed to be strongly concave (we aim to maximize). This is not the case for \acs{RL}, so we have to deal with non-concave objective functions. Then the dataset is not initially available and may even be infinite, which makes approximations unavoidable. This rules out \acs{SAG} and \acs{SAGA} because of their storage requirements, which leaves \acs{SVRG} as the most promising choice.
Finally, the distribution used to sample data is not under direct control of the algorithm designer, but it is a function of policy parameters that changes over time as the policy is optimized, which is a form of non-stationarity. 
\acs{SVRG} has been used in \acs{RL} as an efficient technique to optimize the per-iteration problem in \ac{TRPO} \citep{xu2017stochastic} or for policy evaluation \citep{du2017stochastic}. In both cases, the optimization problems faced resemble the \acs{SL} scenario and are not affected by all the previously mentioned issues.
We will give some theoretical fundamentals for our \acs{SVRG} version, called \acs{SVRPG}, in particular, we will provide convergence guarantees for the proposed algorithm under the assumptions given before. Non-concavity and approximate estimates of the FG have been analysed independently in SL (\cite{allen2016variance},\cite{reddi2016stochastic},\cite{harikandeh2015stopwasting}) but never combined.\newline SVRPG has a convergence rate that has $O(\nicefrac{1}{T})$ dependence on the number $T$ of iterations, up to now (e.g. in \acs{SG}) the convergence rate was  $O(\nicefrac{1}{T^2})$ so this result is of great impact in theoretical \acs{RL}.\newline
 Furthermore, we will propose a practical variant of \acs{SVRPG} and we will show the empirical results of our work in widely known tasks from the literature. We will compare our algorithm with traditional approaches which use traditional \acs{SG}. We show that in practice our technique is better and the learning process is more stable than \acs{SG}.\newline
Our contribution makes available a totally new technique for variance reduction in policy gradient which is orthogonal to the already existing ones (e.g. baselines).
Preliminary results support the effectiveness of SVRPG also with a commonly used baseline for the policy gradient. Despite that, we believe that it will be possible to derive a baseline designed explicitly for SVRPG to jointly exploit the RL structure and the SVRG idea.\newline
This work gives a new direction towards which we can push existing policy gradient optimization techniques stemming different new algorithms beyond those ones proposed by ourselves: applying \acs{SVRPG} over natural gradient techniques, finding a proper way to build a critic suited for \acs{SVRPG} techniques. We expect new efforts coming in the future exploiting our theoretical fundamentals in order to extend our work along the prevously mentioned directions .\newline
The structure of the remaining part of this document the following:
\hyperref[chap:art]{Chapter 2} describes all the state of the art techniques needed to introduce our contribution. Mainly we will talk about policy gradient, natural policy gradient and \acs{SVRG} in \acs{SL}.\newline
\hyperref[chap:art]{Chapter 3} describes \acs{SVRPG}, our proposed algorithm in detail addressing all the difficulties mentioned above. chapter also describes all \acs{SVRPG}'s variants.\newline
\hyperref[chap:art]{Chapter 4} provides convergence guarantees under the presented assumption.\newline
\hyperref[chap:art]{Chapter 5} shows an empirical analysis of the proposed technique in continuous control benchmarks comparing the results with plain \acs{SG}.\newline
\hyperref[chap:art]{Chapter 6} gives a discussion about the whole work providing also some hints about future development.\newline
\hyperref[chap:art]{Appendix} provides all the dimostrations and the experimental details.
