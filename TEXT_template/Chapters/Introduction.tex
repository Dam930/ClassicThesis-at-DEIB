% !TEX root = ../ClassicThesis_DEIB.tex

\chapter{INTRODUCTION} \label{chap:aChapter}

\ac{RL} is a field of machine learning that aims at building intelligent machines, called agent, capable of learning complex tasks from experience.
The tasks in \acs{RL} are represented as a \acs{MDP}s. 
The goal of \acs{RL} \citep{sutton1998reinforcement} algorithm is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal.
This makes \acs{RL} fundamentally different from \ac{SL}, where correct actions are explicitly prescribed by a human teacher (e.g., for classification, in the form of class labels).
However, the two approaches share many challenges and tools: the problem of estimating a model from samples, which is at the core of \acs{SL}, is equally fundamental in \acs{RL}, whether we choose to model the environment, a value function, or directly a policy defining the agent’s behaviour.
A subset of \acs{RL} tasks is composed by continous domains tasks: the states in wich the agent can be and the possible actions it can perform are infinite.
\acs{RL} continuous domains are common in automatic control and robotic applications. Traditional \acs{RL} algorithms, based on value functions, suffer from many problems when applied to continuous tasks, the most serious being the lack of strong theoretical guarantees. Policy gradient methods, besides guaranteeing convergence to locally optimal policies \citep{sutton2000policy}, are able to address many of the difficulties that characterize complex control problems, such as high dimensional state that leads to an infinite space of the cases of interest. 
Many different policy gradient algorithms have been proposed, all sharing the same basic principle of computing the gradient of a performance measure to update the agent’s policy in a direction of improvement. These methods have been successfully applied to complex control tasks \citep{deisenroth2013survey}. Policy gradient is used to improve an existing policy, which may be designed by a human expert or learned by simulation.\newline
In a generic Machine Learning task (\acs{SL},\acs{RL}...), the idea of stochastic gradient (\acs{SG}) ascent \citep{nesterov2013introductory} is to iteratively focus on a random subset of the available data (batch) to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data.
However, the sub-sampling of data is a source of variance that can potentially compromise convergence, so that per-iteration efficiency and convergence rate must be traded off with proper handling of meta-parameters: in \acs{RL} continous task, for example, is difficult to estimate the policy gradient from the few sample we can obtain:  the set of data may even be infinite and the cost for sampling data is high.
Among RL approaches, policy gradient is the one that bears the closest similarity to \acs{SL} solutions. The fundamental principle of these methods is to optimize a parametric policy through \acs{SG}.
Until now, (\acs{SG}) is the unique way used for estimating policy gradient. The approach of increasing the batch size is not an option in \acs{RL} due to the high cost of collecting samples, i.e., by interacting with the environment. In \acs{SL} tasks there are variance-reduced gradient algorithms already used as \acs{SAG} \citep{roux2012stochastic}, \acs{SVRG} \citep{allen2016variance} and \acs{SAGA} \citep{defazio2014saga}. This technique that has proven to be very successful in \acs{SL}.\newline.
The aim of this thesis is to develop a novel approach for reducing the variance of policy gradient's estimate using the \acs{SVRG} adapting it for \acs{RL} tasks.\newline
As mentioned before, compared to other applications of \acs{SG}, the cost of collecting samples can be very high since it requires to interact with the environment.
This makes SVRG-like methods potentially much more efficient than, e.g., batch learning. 
Unfortunately, RL has a series of difficulties that are not present in \acs{SL}. First, in \acs{SL} the objective can often be designed to be strongly concave (we aim to maximize). This is not the case for \acs{RL}, so we have to deal with non-concave objective functions. Then, as mentioned before, the dataset is not initially available and may even be infinite, which makes approximations unavoidable. This rules out \acs{SAG} and \acs{SAGA} because of their storage requirements, which leaves \acs{SVRG} as the most promising choice.
Finally, the distribution used to sample data is not under direct control of the algorithm designer, but it is a function of policy parameters that changes over time as the policy is optimized, which is a form of non-stationarity. 